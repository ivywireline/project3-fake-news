\documentclass[10pt,letterpaper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{listings}
\usepackage{float}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\today}
\chead{Project 2}
\rhead{Tan, Zhou}
%\usepackage[margin=1in]{geometry}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}

\newcommand{\ssbracket}[2]{#1^{(#2)}}

\author{Hao Hui Tan(999741711, tanstev1)\\Kyle Zhou (1000959732, zhoukyle)}
\title{CSC411H1S Project 3}
\begin{document}
	\lstset{language=Python,%
		%basicstyle=\color{red},
		breaklines=true,%
		%morekeywords={matlab2tikz},
		keywordstyle=\color{blue},%
		morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
		identifierstyle=\color{black},%
		stringstyle=\color{mylilas},
		commentstyle=\color{mygreen},%
		showstringspaces=false,%without this there will be a symbol in the places where there is a space
		numbers=left,%
		numberstyle={\tiny \color{black}},% size of the numbers
		numbersep=9pt, % this defines how far the numbers are from the text
		emph=[1]{for,end,break},emphstyle=[1]\color{red}, %some words to emphasise
		%emph=[2]{word1,word2}, emphstyle=[2]{style},
		caption=\lstname,
	}

	\maketitle
	\newpage
	\begin{enumerate}
		\item %1
		The Real headline data set seems to be larger than the Fake headline data set.
		Most of the headlines for both data sets are in English, but there are some French and Spanish headlines, as well as possibly other languages.

		Fake headlines seem to use ``Trump'' to refer to Donald Trump, while real headlines tend to use ``Donald Trump.''
		Fake headlines also tend to use more sensational or inflammatory terms such as declaring something ``an hilarious fail,'' or have grammatical mistakes like the aforementioned example.
		Headlines in general are all lowercase with no punctuation.
		However, it seems that the real headlines tend to be truncated, while the fake headlines seem to all have the full text.
		Some of the headlines are also misspelled (e.g. ``x jinpingi'' instead of ``xi jinping'').

		It is difficult to categorize headlines solely based on keywords, since the same word in different contexts could be either sensational, or factual.
		Some useful keywords could be ``racist'' (5 occurrences in fake, 2 occurrences in real), ``hillary,'' (18 occurrences in real, 97 occurrences in fake), and ``rigged'' (3 occurrences in real, 15 occurrences in fake).

		\item %2
		The Naive Bayes algorithm was computed by computing the conditional probability 
		\[
			P(x_i | c) =  \frac{count(x_i = 1, c)}{count(c)}
		\]
		for all $x_i$ in the training set, and for each class $c$ (i.e. ``real'' or ``fake'').
		The actual formula used involves using $m$ and $\hat{p}$ as priors in order to improve the accuracy of our model, since many words only occur once, or a few times.
		Thus, the formula we trained with was
		\[
			P(x_i | c) =  \frac{count(x_i = 1, c) + m\hat{p}}{count(c) + m}
		\]
		
		To predict whether a headline was real or fake, we computed the conditional probability
		\[P(x_1, ..., x_n | c) P(c)\]
		by computing 
		\[\prod_{i = 1}^{n}p(x_i|y=c)\]
		However, since for the less frequent words, $p(x_i|y=c)$ is very small, and multiplying them together may result in underflow, we computed the exponential of the sum of the log probabilities instead.
		Thus, our formula becomes 
		\[
			P(x_1, ..., x_n | c) P(c) = \exp\left(\sum_{i = 1}^{n}\log(P(x_i|y=c))\right)P(c)
		\]
		We then return the class with the highest probability as our prediction.
		
		In order to tune the $m$ and $\hat{p}$ parameters, we trained the model with varying values, with $m \in [1, 20]$ and $p\in [0.1, 1.0]$, with a step of 1 and 0.1, respectively, and found the values that performed best on the validation set.
		
		The best params are as follows: \\
		\[m = 2, \hat{p} = 0.2\]
		\\
		The code is included below:
		\begin{lstlisting}
def train_model(real_headlines, fake_headlines, m, p):
    word_list = get_wordlist(real_headlines, fake_headlines)
    real_counts = count_word_occurrance(real_headlines)
    fake_counts = count_word_occurrance(fake_headlines)
    probabilities_real = {}
    probabilities_fake = {}
    for word in word_list:
        # if word in ENGLISH_STOP_WORDS: continue
        if word in real_counts:
            probabilities_real[word] = (real_counts[word] + m * p) / float(len(real_headlines) + m)
        else:
            probabilities_real[word] = (0 + m * p) / float(len(real_headlines) + m)
        if word in fake_counts:
            probabilities_fake[word] = (fake_counts[word] + m * p) / float(len(fake_headlines) + m)
        else:
            probabilities_fake[word] = (0 + m * p) / float(len(fake_headlines) + m)

    return probabilities_real, probabilities_fake, m, p, len(real_headlines), len(fake_headlines), word_list

def predict_model(model, headline):
    probabilities_real, probabilities_fake, m, p, real_count, fake_count, word_list = model
    logprob_real = 0.0
    logprob_fake = 0.0
    real_class_prob = float(real_count) / (real_count + fake_count)
    fake_class_prob = float(fake_count) / (real_count + fake_count)
    headline_split = headline.split(' ')
    for word in word_list:
        # if word in ENGLISH_STOP_WORDS: continue
        if word in headline_split:
            logprob_real += math.log(probabilities_real[word])
            logprob_fake += math.log(probabilities_fake[word])
        else:
            logprob_real += math.log(1 - probabilities_real[word])
            logprob_fake += math.log(1 - probabilities_fake[word])
    real_prob = math.exp(logprob_real) * real_class_prob
    fake_prob = math.exp(logprob_fake) * fake_class_prob
    # print real_prob, fake_prob
    return real_prob > fake_prob

def tune_model(real_training, fake_training, real_validation, fake_validation):
    performance_report = {}
    m = 1
    while m <= 20:
        p = 0.1
        while p <= 1:
            model = train_model(real_training, fake_training, m, p)
            performance = get_performance(model, real_validation, fake_validation)
            print m, p, performance
            performance_report[(m, p)] = performance
            p += 0.1
        m += 1

    print "The m and p value is", max(performance_report, key=performance_report.get)

    return performance_report

def get_performance(model, real, fake):
    correct = 0

    for hl in real:
        if predict_model(model, hl):
            correct += 1

    for hl in fake:
        if not predict_model(model, hl):
            correct += 1

    return float(correct) / (len(real) + len(fake))
		\end{lstlisting}
		\item %3
		\begin{enumerate}
			\item %3a
			\item %3b
			\item %3c
		\end{enumerate}
	\item %4
	\item %5
	\item %6
	\begin{enumerate}
		\item %6a
		\item %6b
		\item %6c
	\end{enumerate}
	\item %7
	\begin{enumerate}
		\item %7a
		\item %7b
		\item %7c
	\end{enumerate}
	\item %8
	\begin{enumerate}
		\item %8a
		\item %8b
	\end{enumerate}
	\end{enumerate}
\end{document}
