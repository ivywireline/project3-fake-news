\documentclass[10pt,letterpaper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{listings}
\usepackage{float}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\today}
\chead{Project 3}
\rhead{Tan, Zhou}
%\usepackage[margin=1in]{geometry}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}

\newcommand{\ssbracket}[2]{#1^{(#2)}}

\author{Hao Hui Tan(999741711, tanstev1)\\Kyle Zhou (1000959732, zhoukyle)}
\title{CSC411H1S Project 3}
\begin{document}
	\lstset{language=Python,%
		%basicstyle=\color{red},
		breaklines=true,%
		%morekeywords={matlab2tikz},
		keywordstyle=\color{blue},%
		morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
		identifierstyle=\color{black},%
		stringstyle=\color{mylilas},
		commentstyle=\color{mygreen},%
		showstringspaces=false,%without this there will be a symbol in the places where there is a space
		numbers=left,%
		numberstyle={\tiny \color{black}},% size of the numbers
		numbersep=9pt, % this defines how far the numbers are from the text
		emph=[1]{for,end,break},emphstyle=[1]\color{red}, %some words to emphasise
		%emph=[2]{word1,word2}, emphstyle=[2]{style},
		caption=\lstname,
	}

	\maketitle
	\newpage
	\begin{enumerate}
		\item %1
		The Real headline data set seems to be larger than the Fake headline data set.
		Most of the headlines for both data sets are in English, but there are some French and Spanish headlines, as well as possibly other languages.

		Fake headlines seem to use ``Trump'' to refer to Donald Trump, while real headlines tend to use ``Donald Trump.''
		Fake headlines also tend to use more sensational or inflammatory terms such as declaring something ``an hilarious fail,'' or have grammatical mistakes like the aforementioned example.
		Headlines in general are all lowercase with no punctuation.
		However, it seems that the real headlines tend to be truncated, while the fake headlines seem to all have the full text.
		Some of the headlines are also misspelled (e.g. ``x jinpingi'' instead of ``xi jinping'').

		It is difficult to categorize headlines solely based on keywords, since the same word in different contexts could be either sensational, or factual.
		Some useful keywords could be ``racist'' (5 occurrences in fake, 2 occurrences in real), ``hillary,'' (18 occurrences in real, 97 occurrences in fake), and ``rigged'' (3 occurrences in real, 15 occurrences in fake).

		\item %2
		The Naive Bayes algorithm was computed by computing the conditional probability
		\[
			P(x_i | c) =  \frac{count(x_i = 1, c)}{count(c)}
		\]
		for all $x_i$ in the training set, and for each class $c$ (i.e. ``real'' or ``fake'').
		The actual formula used involves using $m$ and $\hat{p}$ as priors in order to improve the accuracy of our model, since many words only occur once, or a few times.
		Thus, the formula we trained with was
		\[
			P(x_i | c) =  \frac{count(x_i = 1, c) + m\hat{p}}{count(c) + m}
		\]

		To predict whether a headline was real or fake, we computed the conditional probability
		\[P(x_1, ..., x_n | c) P(c)\]
		by computing
		\[\prod_{i = 1}^{n}p(x_i|y=c)\]
		However, since for the less frequent words, $p(x_i|y=c)$ is very small, and multiplying them together may result in underflow, we computed the exponential of the sum of the log probabilities instead.
		Thus, our formula becomes
		\[
			P(x_1, ..., x_n | c) P(c) = \exp\left(\sum_{i = 1}^{n}\log(P(x_i|y=c))\right)P(c)
		\]
		We then return the class with the highest probability as our prediction.

		In order to tune the $m$ and $\hat{p}$ parameters, we trained the model with varying values, with $m \in [1, 20]$ and $p\in [0.1, 1.0]$, with a step of 1 and 0.1, respectively, and found the values that performed best on the validation set.

		The performance of the classifier on the training set is 96\%, the performance on the validation set is 85\%, and the performance on the test set is 85\%

		The best params are as follows: \\
		\[m = 2, \hat{p} = 0.2\]
		\\
		The code is included below:
		\begin{lstlisting}
def train_model(real_headlines, fake_headlines, m, p):
    word_list = get_wordlist(real_headlines, fake_headlines)
    real_counts = count_word_occurrance(real_headlines)
    fake_counts = count_word_occurrance(fake_headlines)
    probabilities_real = {}
    probabilities_fake = {}
    for word in word_list:
        # if word in ENGLISH_STOP_WORDS: continue
        if word in real_counts:
            probabilities_real[word] = (real_counts[word] + m * p) / float(len(real_headlines) + m)
        else:
            probabilities_real[word] = (0 + m * p) / float(len(real_headlines) + m)
        if word in fake_counts:
            probabilities_fake[word] = (fake_counts[word] + m * p) / float(len(fake_headlines) + m)
        else:
            probabilities_fake[word] = (0 + m * p) / float(len(fake_headlines) + m)

    return probabilities_real, probabilities_fake, m, p, len(real_headlines), len(fake_headlines), word_list

def predict_model(model, headline):
    probabilities_real, probabilities_fake, m, p, real_count, fake_count, word_list = model
    logprob_real = 0.0
    logprob_fake = 0.0
    real_class_prob = float(real_count) / (real_count + fake_count)
    fake_class_prob = float(fake_count) / (real_count + fake_count)
    headline_split = headline.split(' ')
    for word in word_list:
        # if word in ENGLISH_STOP_WORDS: continue
        if word in headline_split:
            logprob_real += math.log(probabilities_real[word])
            logprob_fake += math.log(probabilities_fake[word])
        else:
            logprob_real += math.log(1 - probabilities_real[word])
            logprob_fake += math.log(1 - probabilities_fake[word])
    real_prob = math.exp(logprob_real) * real_class_prob
    fake_prob = math.exp(logprob_fake) * fake_class_prob
    # print real_prob, fake_prob
    return real_prob > fake_prob

def tune_model(real_training, fake_training, real_validation, fake_validation):
    performance_report = {}
    m = 1
    while m <= 20:
        p = 0.1
        while p <= 1:
            model = train_model(real_training, fake_training, m, p)
            performance = get_performance(model, real_validation, fake_validation)
            print m, p, performance
            performance_report[(m, p)] = performance
            p += 0.1
        m += 1

    print "The m and p value is", max(performance_report, key=performance_report.get)

    return performance_report

def get_performance(model, real, fake):
    correct = 0

    for hl in real:
        if predict_model(model, hl):
            correct += 1

    for hl in fake:
        if not predict_model(model, hl):
            correct += 1

    return float(correct) / (len(real) + len(fake))
		\end{lstlisting}
		\item %3
		\begin{enumerate}
			\item %3a
			The 10 words whose presence most strongly predicts that the news is real
			\begin{enumerate}
				\item japan
				\item daniel
				\item refugee
				\item denies
				\item zoe
				\item charlottesville
				\item korea
				\item business
				\item nfl
				\item south
			\end{enumerate}

			The 10 words whose absence most strongly predicts that the news is real
			\begin{enumerate}
				\item trump
				\item the
				\item to
				\item hillary
				\item a
				\item is
				\item and
				\item for
				\item clinton
				\item in
			\end{enumerate}

			The 10 words whose presence most strongly predicts that the news is fake
			\begin{enumerate}
				\item hats
				\item debunks
				\item sleep
				\item hating
				\item battleground
				\item haters
				\item pointing
				\item dazu
				\item pantano
				\item amtsantritt
			\end{enumerate}

			The 10 words whose absence most strongly predicts that the news is fake
			\begin{enumerate}
				\item donald
				\item trumps
				\item us
				\item says
				\item ban
				\item korea
				\item north
				\item turnbull
				\item travel
				\item australia
			\end{enumerate}

			These lists were obtained by finding the words with the max values for the conditional probabilities
			\[
				P(\text{class} | \text{word}) = \frac{P(\text{word} | \text{class})P(\text{class})}{P(\text{word})}
			\]
			where
			\[
				P(\text{word}) = \frac{\text{count}(\text{word}) + m\hat{p}}{\text{\text{count}}(\text{headlines}) + m}
			\]
			and
			\[
			P(\text{class} | \neg\text{word}) = \frac{P(\neg\text{word} | \text{class})P(\text{class})}{P(\neg\text{word})}
			\]
			\[
			= \frac{(1 - P(\text{word} | \text{class}))P(\text{class})}{1 - P(\text{word})}
			\]
			The words that have the biggest sway on whether a headline is real or fake tend to be words that only appear in one set and not the other.
			However, the presence of a word in one class's top 10 doesn't imply that it will be as strong when absent from the other class.


			%(model.probs_real[k] * prob_real) / (model.real_word_counts[k] + model.m / float(model.real_word_counts[k] + model.fake_word_counts[k]))
			%If the conditional probability is high, then if the word is present in the headline, it will not decrease the term $P(x_1, ..., x_n | c)$ as much (since the probability is closer to 1), which means that the final conditional probability will be higher.
			%If the conditional probability is low, then if the word is \textit{absent} from the headline, it will also result in a lower penalty than if the probability were higher.
			%That's because $1 - P(x_i|c)$ is used instead.
			For the code, please see \verb|get_top_bottom_word_occurrences| in fake.py.

			%However, due to the addition of the $m$ and $\hat{p}$ values to the equation for $p(x_i|c)$, the bounds on the probability is not 1.0 and 0.0.
			%In the case of our optimal values ($m = 2, \hat{p} = 0.2$), the min and max values are  0.00392 and 0.984, respectively.
			%Since \(1 - 0.00392 > 0.984\), having a missing word absent is stronger than having a common word present.

			\item %3b
			The following lists have had the stopwords from scikit\_learn excluded

			The 10 words whose presence most strongly predicts that the news is real
			\begin{enumerate}
				\item japan
				\item daniel
				\item refugee
				\item denies
				\item zoe
				\item charlottesville
				\item korea
				\item business
				\item nfl
				\item south
			\end{enumerate}

			The 10 words whose absence most strongly predicts that the news is real
			\begin{enumerate}
				\item trump
				\item hillary
				\item clinton
				\item just
				\item america
				\item supporters
				\item vote
				\item black
				\item win
				\item watch
			\end{enumerate}

			The 10 words whose presence most strongly predicts that the news is fake
			\begin{enumerate}
				\item hats
				\item debunks
				\item sleep
				\item hating
				\item battleground
				\item haters
				\item pointing
				\item dazu
				\item pantano
				\item amtsantritt
			\end{enumerate}

			The 10 words whose absence most strongly predicts that the news is fake
			\begin{enumerate}
				\item donald
				\item trumps
				\item says
				\item ban
				\item korea
				\item north
				\item turnbull
				\item travel
				\item australia
				\item climate
			\end{enumerate}
			\item %3c
			It might make sense to remove stopwords when looking at the ratio of word frequency between the real and fake headlines, because these words are common, and would likely appear in most if not all headlines.
			These words are also uninteresting on their own, and so they crowd up the word frequency rankings, possibly pushing more interesting words down the list.
			Another reason for removing stopwords would be when the input set includes headlines of different languages.
			This is because stopwords in one language are usually uncommon in other languages, and once again skew the results.

			It would make sense to keep stopwords because their presence could still help distinguish between classes, because one class might use certain stopwords in a different way, or use them more (or less) frequently than the other class.
			Furthermore, in testing, we have found that training while excluding stopwords does not necessarily increase the performance of our model.

		\end{enumerate}
	\item %4
	The learning curves of Logistic Regression can be seen in Figure \ref{fig:part4learningcurve}.
	\begin{figure}[H]
		\centering
		\includegraphics[width=\linewidth]{Part4LearningCurve}
		\caption{Performance of Logistic Regression vs. Number of Epochs}
		\label{fig:part4learningcurve}
	\end{figure}

	% talk about how you chose the regularization parameter
	We used L2 regularization to prevent overfitting.
	We chose the $\lambda$ value by training the model on the training set with varying $\lambda$ values, and choosing the value that had the highest performance on the validation set.
	The value that seemed to perform the best was 0.007.

	When searching for the $\lambda$ value, we started off with small values and a small step size.
	This is because our performance on the validation set without L2 regularization was already quite good, and an excessively large lambda value would penalize the data too much.

	\item %5
	\item %6
	\begin{enumerate}
		\item %6a
		The top positive words in the logistic regression model are as follows:
		\begin{enumerate}
			\item nfl
			\item australia
			\item ban
			\item defends
			\item over
			\item week
			\item appears
			\item law
			\item back
			\item pence
		\end{enumerate}
		This list contains a combination of the 10 words whose presence most strongly predicts that the news is real, and the 10 words whose absence most strongly predicts that the news is fake.
		For example, ``nfl'' can be found in the former list from 3(a), and ``australia,'' and ``ban'' can be found in the latter list.

		The top negative words in the logistic regression model are as follows:
		\begin{enumerate}
			\item this
			\item reason
			\item paid
			\item hillary
			\item have
			\item it
			\item 5
			\item votes
			\item are
			\item need
		\end{enumerate}
		This list should contain a combination of the 10 words whose presence most strongly predicts that the news is fake, and the 10 words whose absence most strongly predicts that the news is real.
		For example, ``hillary'' can be found in the former list from 3(a).
		However, no words can be found in the latter list.
		This is likely due to stop words pushing the other words down the list.

		Notably, the stopwords contained in these lists are not the same.

		\item %6b
		The top positive words in the logistic regression model excluding stopwords are as follows:
		\begin{enumerate}
			\item nfl
			\item australia
			\item ban
			\item defends
			\item week
			\item appears
			\item law
			\item pence
			\item senate
			\item push
		\end{enumerate}
		Again, this is a combination of the top 10 words whose presence most strongly predicts that the news is real, and the top 10 words whose absence most strongly predicts that the news is fake.
		The similarities are the same as in 6(a).

		The top negative words in the logistic regression model excluding stopwords are as follows:
		\begin{enumerate}
			\item reason
			\item paid
			\item hillary
			\item 5
			\item votes
			\item need
			\item goes
			\item f
			\item won
			\item riots
		\end{enumerate}
		Again, this is a combination of the top 10 words whose presence most strongly predicts that the news is fake, and the top 10 words whose absence most strongly predicts that the news is real.
		The similarities are the same as in 6(a).
		There seem to be some additional synonyms that correspond (e.g. ``votes'' vs. ``vote''), but not additional exact matches.
		\item %6c
		It might not be the best idea to read the logistic regression parameters directly because logistic regression does not require normalized input.
		If there was a value that was abnormally high or low in the training set, the weight corresponding to that value would be small to compensate.
		That value could be quite significant, but since the data is not normalized, those weights would be lower than one would expect.
		This is not a concern in our case since our data sets are all normalized (the only values the features could take are 0 and 1).
		%because if they're not normalized because this would not be representative of words whose combination is stronger than the word individually.
		%Due to this, headlines in the training set with large amounts of words would influence the weights in a way so that words in combination would be strong, but the words individually may not be as strong.
		%This is not an issue in our case, since our headlines tend to be of similar length.
		%Furthermore, since Logistic regression assumes that
		\[
			\log\frac{p(\text{true}|x,\theta)}{p(\text{fake}|x,\theta)} = \theta_0 + \theta_1x_1 + ... + \theta_dx_d
		\]
	\end{enumerate}
	\item %7
	\begin{enumerate}
		\item %7a

	The following graph shows the relationship between \textbf{max\_depth} values and the accuracy of the training and validation sets.
	\begin{figure}[H]
		\centering
		\includegraphics[width=\linewidth]{Part7MaxDepthGraph}
		\caption{Performance of Training and Validations sets and Max\_depth values}
		\label{fig:part4learningcurve}
	\end{figure}

	As one can see from the graph, the best max\_depth values to use is 25 as the Decision Tree classifier reaches the best performance at that value for the validation set. In addition to max depth values, I have also tried out different criterion values. I have tested out Gini and Entropy criterion and found that the Entropy criterion has better performance than the former. I have also tried out different min\_sample\_leaf values and settled on 15 because it gives better values than the rest. If the minimum sample leaf value is too small, it will likely to cause over-fitting as each branch may not have enough data for the algorithm to generalize well. If the value is too large, then it will prevent the model from learning at all.
		\item %7b

		Here is the visualization of the first two layers of the decision tree.
	\begin{figure}[H]
		\centering
		\includegraphics[width=\linewidth]{part7b_first_two_layers.png}
		\caption{Visualization of the first 2 layers of the Decision Tree}
		\label{fig:part4learningcurve}
	\end{figure}

	The most important keyword features in the Naive Bayes from 3a are \newline
	Positive indication that headline is real:
			\begin{enumerate}
				\item hats
				\item debunks
				\item sleep
				\item hating
				\item battleground
				\item haters
				\item pointing
				\item dazu
				\item pantano
				\item amtsantritt
			\end{enumerate}
	Negative indicating that headline is real: \newline
			\begin{enumerate}
				\item donald
				\item trumps
				\item us
				\item says
				\item ban
				\item korea
				\item north
				\item turnbull
				\item travel
				\item australia
			\end{enumerate}

	The most important keyword features for the logistic regression model are: \newline

	Positive indication that it's real:
			\begin{enumerate}
			\item nfl
			\item australia
			\item ban
			\item defends
			\item over
			\item week
			\item appears
			\item law
			\item back
			\item pence
		\end{enumerate}

	Negative indication that it's real:
		\begin{enumerate}
			\item this
			\item reason
			\item paid
			\item hillary
			\item have
			\item it
			\item 5
			\item votes
			\item are
			\item need
		\end{enumerate}

	The most important keyword features for the decision tree are 'donald', 'trumps', 'the', 'hillary', 'on', 'a', 'of'.
	The keywords donald and trumps are in the set of important features in naive bayes. The keyword hillary is in the set of important features in logistic regression. It is interesting that all these words are negative indications that the headline is real, i.e they are fake news predictors.
		\item %7c
	The performance of the Naive Bayes is $96\%$ on the training set, $85\%$ on the validation set, $85\%$ on the test set.

	The performance of the Logistic Regression is $94\%$ on the training set, $80\%$ on the validation set $80\%$ on the test set.

	The performance of the Decision Tree is $76.2\%$ on the training set, $74.8\%$ on the validation set $75.2\%$ on the test set.

	Naive Bayes performed the best according to the data. However, it is a close between Naive Bayes and Logistic Regression. The Decision Tree algorithm performed the worst. Surprising, logistic regression over-fitted the most. In theory, it should be decision that is the most likely to over-fit and Naive Bayes that is the least likely to over-fit.

	\end{enumerate}
	\item %8
	\begin{enumerate}
		\item %8a

		$I(Y, x_{1}) = H(Y) - \sum_{k}{P(X=x_{k})H(Y|X=x_{k})}
		             = 0.97 - ((.67)(0.999) + (0.33)(0.747))
		             = 0.054$
		Thus the mutual information of the first split is 0.054.

		\item %8b
		$I(Y, x_{2}) = H(Y) - \sum_{k}{P(X=x_{k})H(Y|X=x_{k})}
		             = 0.999 - ((.93)(0.999) + (0.07)(0.14))
		             = 0.06$
	    I chose the first child of the first split as the variable for this question. The mutual information on this variable is slightly larger than the first split. This indicates that the this variable tells us more information about whether the headline is real or fake than the first headline.
	\end{enumerate}
	\end{enumerate}
\end{document}
